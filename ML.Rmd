---
output:
  bookdown::pdf_document2: default
classoption: landscape
---
# Step 4 Machine learning

\newpage
## Step 0: Look at and Modify the dataset
So, I am curious. Can I predict vaccination data?

```{r python, include=FALSE}
# Setting for using python
library(tidyverse)
library(reticulate)
use_python("/Users/travel_mechtal/opt/anaconda3/bin/python")
use_condaenv("base", required = TRUE)

# if I need a new module
#py_install("ModuleName")
# I already installed: pandas, networkx, typing_extensions, joblib, seaborn, scikit-learn
```

```{python python_libraries, include=FALSE}
# work with dataframes
import pandas as pd
# work with dates
import datetime as dt
# split the dataset
from sklearn.model_selection import train_test_split
# evaluate the model
from sklearn.metrics import mean_absolute_error
# graph data
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.tree import export_text
# build a model
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
```

```{r ML_r_libraries, include=FALSE}
#---------------------------------------
# ggplot2 -- plot
#---------------------------------------
if(!require(ggplot2)){install.packages("ggplot2")}
library(ggplot2)
#---------------------------------------
# reshape2 -- use melt
#---------------------------------------
if(!require(reshape2)){install.packages("reshape2")}
library(reshape2)
#---------------------------------------
# ggplot2 -- plot
#---------------------------------------
if(!require(ggplot2)){install.packages("ggplot2")}
library(ggplot2)
#---------------------------------------
# psych -- describing data
#---------------------------------------
if(!require(psych)){install.packages("psych")}
library(psych)
#---------------------------------------
# knitr, kableExtra -- describing data
#---------------------------------------
if(!require(knitr)){install.packages("knitr")}
library(knitr)
if(!require(kableExtra)){install.packages("kableExtra")}
library(kableExtra)
```

```{python read_data, include=FALSE}
# read file
path = "/Users/travel_mechtal/Documents/UWE/Portfolio/"
data=pd.read_csv(path + "region_2022-01-27.csv", index_col="date", parse_dates=True)

# drop unnecessary columns: areaCode, areaName, areaType.
dataset = data.drop(["areaCode", "areaName", "areaType"], axis=1)

# rename columns
# newPeopleVaccinatedFirstDoseByVaccinationDate -> First, 
# newPeopleVaccinatedSecondDoseByVaccinationDate -> Second, 
# newPeopleVaccinatedThirdInjectionByVaccinationDate -> Third
dataset = dataset.rename(columns={"newPeopleVaccinatedFirstDoseByVaccinationDate":"First", 
                                  "newPeopleVaccinatedSecondDoseByVaccinationDate":"Second", 
                                  "newPeopleVaccinatedThirdInjectionByVaccinationDate":"Third"})
# Replace Na values                                  
dataset = dataset.fillna(0)   
```

I will work with the South West's vaccination data.
```{r DataRegions, eval=TRUE, echo=FALSE}
kable(py$dataset[1:10,])
``` 

```{python DataRegions_all_years, fig.align = "center", echo=FALSE}
plt.figure(figsize=(20,12))
sns.heatmap(data=
            dataset.sort_index())
```
As we can see, there are waves. So, the count of jabs depends on dates.

Let's get features:
1) Year
2) Month
3) Day
etc.

```{python DateFeatures, include=FALSE}
dataset['Year'] = dataset.index.year
dataset['Month'] = dataset.index.month
dataset['Day'] = dataset.index.day
dataset['DayOfYear'] = dataset.index.dayofyear
dataset['Weekday'] = dataset.index.weekday
dataset['Quarter'] = dataset.index.quarter
dataset['IsMonthStart'] = dataset.index.is_month_start
dataset['IsMonthEnd'] = dataset.index.is_month_end
```

```{r DataRegions_features, eval=TRUE, echo=FALSE} 
kable(py$dataset[1:10,])
```

First of all, I am going to use Regression Machine Learning models: 

- [Decision Tree](https://scikit-learn.org/stable/modules/tree.html#regression) 

- [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).

Let's look at the dataset carefully.
\newpage

## Step 1: Explore the dataset

### Data types
```{r, echo=FALSE, eval=TRUE, echo=FALSE}
kable(sapply(py$dataset, typeof))
```

The good news is I don't need to convert my variables because they fit into Regression Machine Learning models.

\newpage
### Data description
```{r, long_dataset, include=FALSE}
dataset_long <- melt(py$dataset
                       , measure.vars = c("First", "Second", "Third")
                       , variable.name = "Dose"
                       , value.name = "Count"
                    )
```

```{r, include=FALSE}
dataset_desc <- describe(py$dataset)
```

_Median and mean_

```{r, echo=FALSE, eval=TRUE, echo=FALSE}
kable(dataset_desc[c("First", "Second", "Third"), c("mean", "median")])
```

Mean and median have a visible difference. What does it mean? There are large values that influence mean values.

```{r, echo=FALSE, fig.align = "center"}
dataset_long %>%
ggplot(aes(x=Count, fill=Dose)) +
    geom_histogram( color="#e9ecef", bins = 15, position = 'identity') +
  facet_grid(Dose ~ .)
```

\newpage
### Zeroes

```{r, echo=FALSE}
dataset_long <- dataset_long %>% mutate(zero =
           case_when(Count == 0 ~ "0", 
                     Count > 0 ~ "> 0")        
           )          
```

```{r, echo=FALSE, fig.align = "center", fig.height = 3, fig.width = 12}
ggplot(dataset_long %>%
        group_by(Dose, zero) %>%
        summarise(zero_count=length(Count))
, aes(x="", y=zero_count, fill=zero)) +
  geom_bar(stat="identity", width=10) +
  coord_polar("y", start=0) +
  facet_grid(cols = vars(Dose)) +
  theme_void() # remove background, grid, numeric labels
```

The column "Third" has more zero values than "First" and "Second; but, I think, it won't influence models' accuracy.

\newpage
### Correlations
```{r, echo=FALSE, fig.align = "center"}
melted_cormat <- melt(cor(py$dataset))
ggplot(data = filter(melted_cormat, Var1 %in% c("First", "Second")), aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
   scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  geom_hline(yintercept = "Month", color = "blue", linetype = "dotted") +
  geom_hline(yintercept = "DayOfYear", color = "blue", linetype = "dotted") +
  geom_hline(yintercept = "Quarter", color = "blue", linetype = "dotted")
```


```{r, echo=FALSE, fig.align = "center"}
kable(filter(melted_cormat, Var1 %in% c("First", "Second"), Var2 %in% c("Month", "DayOfYear", "Quarter")))
```

As we can see, the column "First" has strong relationships with 

- "Quarter", 

- "DayOfYear",

- "Month". 

At the same time, the column "Second" doesn't have strong relationships; but we can use the same columns.

\newpage
### Weekdays
As you remember, I have a question.
```{r ref0, ref.label='q2', render=pander::pander, echo=FALSE}
```

Let's answer.
```{r, echo=FALSE, fig.align = "center"}
dataset_weekdays <- 
  py$dataset %>%
  group_by(Weekday)%>%
  summarise(across(c("First", "Second", "Third"), ~ mean(.x, na.rm = TRUE)))

dataset_weekdays$Weekday <- factor(dataset_weekdays$Weekday, labels = c('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'))

dataset_weekdays_long <-
  melt(dataset_weekdays, id.vars = c("Weekday")
                       , measure.vars = c("First", "Second", "Third")
                       , variable.name = "Dose"
                       , value.name = "Count"
                    )

ggplot(data = dataset_weekdays_long, 
       aes(x=Weekday, 
           y=Count,
           fill=Count
           )
         ) +
  geom_col(position = "dodge") +
  geom_vline(xintercept = "Saturday", color = "red", linetype = "dotted") +
  facet_grid(Dose ~ .)
```

So, most of South West's people prefer to get a jab on Saturdays.

\newpage
### Missing values
Calculate a count of dates in the dataset.
```{python, echo=FALSE}
len(dataset.index)
```
Calculate a count of dates between maximum and minimum dates.
```{python, echo=FALSE}
dates = pd.date_range(dataset.index.min(),dataset.index.max(),freq='d')
len(dates)
```
There are no missing dates.

\newpage
## Step 2: Split sets, train a Machine Learning Model and Evaluate performance
```{r python_functions, include=FALSE}
# function for preparing sets
source_python('prepare_sets.py')
# function for training models
source_python('train_model.py')
```

Define necessary variables
```{python all_columns, include=FALSE}
feature_columns = ["Year", "Month", "Day", "DayOfYear", "Weekday", "Quarter", "IsMonthStart", "IsMonthEnd"]
```

Prepare sets and train models using parameters.
```{python first}
y_column = "First"
```

```{python train_all, echo=FALSE}
train_X, val_X, train_y, val_y = prepare_sets(dataset, feature_columns, y_column)

# DecisionTree
mae_dt, predictions_dt, model_dt = train_model(train_X, val_X, train_y, val_y, "DecisionTree", n_estimators=None)
# RandomForest
mae_rf, predictions_rf, model_rf = train_model(train_X, val_X, train_y, val_y, "RandomForest", n_estimators=500)

print("DecisionTree: ", 1 - mae_dt/dataset[y_column].mean())
print("RandomForest: ", 1 - mae_rf/dataset[y_column].mean())
```

Look at the tree
```{python tree_dt, echo=FALSE, fig.align = "center"}
plt.figure(figsize=(60, 40))
# feature_columns is defined above
tree.plot_tree(model_dt, max_depth=2, feature_names=feature_columns)
```

```{python tree_rf, echo=FALSE, fig.align = "center"}
plt.figure(figsize=(60,40))
# feature_columns is defined above
tree.plot_tree(model_rf.estimators_[0], max_depth=2, feature_names=feature_columns)
```

```{python plot_all, echo=FALSE, fig.align = "center"}
plt.figure(figsize=(25,15))
sns.set_style("darkgrid")
plt.title('{} ({})'.format("Comparing", y_column))
# plot DecisionTree
sns.lineplot(data=predictions_dt, label='{} MAE:{}'.format("DecisionTree", round(mae_dt,0)))
# plot RandomForest
sns.lineplot(data=predictions_rf, label='{} MAE:{}'.format("RandomForest", round(mae_rf,0)))
# plot validation set
val_y.index=range(0,len(val_y))
sns.lineplot(data=val_y, label="Validation")
# add legend
plt.legend()
```

```{python my_columns, include=FALSE}
feature_columns = ["Weekday", "Year", "DayOfYear"]
```

```{python, echo=FALSE}
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```


Repeat for the Second
```{python second_train}
y_column = "Second"
```
```{python, echo=FALSE}
<<columns_all>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```
\newpage
```{python, echo=FALSE}
<<columns_my>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

Repeat for Third
```{python third_train}
y_column = "Third"
```
```{python, echo=FALSE}
<<columns_all>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```
\newpage
```{python, echo=FALSE}
<<columns_my>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

Compare the score with the mean value of the column that we predicted.

A combination of the following features give us the best result: Weekday, Year, DayOfYear.






