---
site: bookdown::bookdown_site
output:
  bookdown::pdf_document2:
    fig_caption: yes
    toc: yes
    latex_engine: xelatex
header-includes:
   - \usepackage{floatrow}
   - \floatsetup[figure]{capposition=top}
classoption: landscape
---
# Step 4 Machine learning

## South West

### Look at and Modify the dataset
So, I am curious. Can I predict vaccination data?

```{r python, include=FALSE}
# Setting for using python
library(tidyverse)
library(reticulate)
use_python("/Users/travel_mechtal/opt/anaconda3/bin/python")
use_condaenv("base", required = TRUE)

# if I need a new module
#py_install("ModuleName")
# I already installed: pandas, networkx, typing_extensions, joblib, seaborn, scikit-learn
```

```{python python_libraries, include=FALSE}
# work with dataframes
import pandas as pd
# work with dates
import datetime as dt
# split the dataset
from sklearn.model_selection import train_test_split
# evaluate the model
from sklearn.metrics import mean_absolute_error
# graph data
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.tree import export_text
# build a model
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
```

```{r ML_r_libraries, include=FALSE}
#---------------------------------------
# ggplot2 -- plot
#---------------------------------------
if(!require(ggplot2)){install.packages("ggplot2")}
library(ggplot2)
#---------------------------------------
# reshape2 -- use melt
#---------------------------------------
if(!require(reshape2)){install.packages("reshape2")}
library(reshape2)
#---------------------------------------
# ggplot2 -- plot
#---------------------------------------
if(!require(ggplot2)){install.packages("ggplot2")}
library(ggplot2)
#---------------------------------------
# knitr, kableExtra -- describing data
#---------------------------------------
if(!require(knitr)){install.packages("knitr")}
library(knitr)
if(!require(kableExtra)){install.packages("kableExtra")}
library(kableExtra)
```

```{r, include=FALSE}
# for pretty tables
source("my_kable.R")
```

```{python read_data, include=FALSE}
# read file
path = "/Users/travel_mechtal/Documents/UWE/Portfolio/"
data=pd.read_csv(path + "region_2022-01-27.csv", index_col="date", parse_dates=True)

# drop unnecessary columns: areaCode, areaName, areaType.
dataset = data.drop(["areaCode", "areaName", "areaType"], axis=1)

# rename columns
# newPeopleVaccinatedFirstDoseByVaccinationDate -> First, 
# newPeopleVaccinatedSecondDoseByVaccinationDate -> Second, 
# newPeopleVaccinatedThirdInjectionByVaccinationDate -> Third
dataset = dataset.rename(columns={"newPeopleVaccinatedFirstDoseByVaccinationDate":"First", 
                                  "newPeopleVaccinatedSecondDoseByVaccinationDate":"Second", 
                                  "newPeopleVaccinatedThirdInjectionByVaccinationDate":"Third"})
# Replace Na values                                  
dataset = dataset.fillna(0)   
```

I will work with the South West's vaccination data.
```{r DataRegions, eval=TRUE, echo=FALSE}
kable(py$dataset[1:10,])
``` 

```{python DataRegions_all_years, fig.align = "center", echo=FALSE}
plt.figure(figsize=(20,12))
sns.heatmap(data=
            dataset.sort_index())
```
As we discuss earlier \@ref(fig:DosesWaves), there are waves. So, the count of jabs depends on dates.

Let's get features:
1) Year
2) Month
3) Day
etc.

```{python DateFeatures, include=FALSE}
dataset['Year'] = dataset.index.year
dataset['Month'] = dataset.index.month
dataset['Day'] = dataset.index.day
dataset['DayOfYear'] = dataset.index.dayofyear
dataset['Weekday'] = dataset.index.weekday
dataset['Quarter'] = dataset.index.quarter
dataset['IsMonthStart'] = dataset.index.is_month_start
dataset['IsMonthEnd'] = dataset.index.is_month_end
```

```{r DataRegionsWithFeatures, ref.label=c('DataRegions'), echo=FALSE}
```

First of all, I am going to use Regression Machine Learning models: 

- [Decision Tree](https://scikit-learn.org/stable/modules/tree.html#regression) 

- [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).

What is my plan?

1. Read data

I already did this step.

2.  Understand statistics about the data 

It will be helpful to choose the right features for better results. 

- Work with missing data and categorical variables
- Work with outliers or not completed data.

5.  Store prediction target (y) in a Series,
selecting multiple features by providing a list of column names inside brackets, 
define X (subset with features),
check the X summary.

6. Choose the library

7. Build and use the model
What type of model will it be?
Capture patterns from provided data.
Predict
Evaluate = Determine how accurate the model’s predictions are

Let's look at the dataset carefully.
\newpage

### Explore the dataset

In the previous chapter \@ref(region), we already looked at the South West's data. Do we need to know something else? Yes.

#### Data types

It is important to know which types of data columns have. Sometimes we don’t realise what we see: the string or the number.
```{r, echo=FALSE, eval=TRUE, echo=FALSE}
kable(sapply(py$dataset, typeof), col.names = NULL)
```

The good news is I don't need to convert my variables because they fit into Regression Machine Learning models.

We will move on to correlations.

\newpage
#### Correlations

What do we need to remember? Correlation does not imply causation. So, the columns that have a strong relationship may show low accuracy in the model.

```{r, echo=FALSE, fig.align = "center"}
melted_cormat <- melt(cor(py$dataset))
ggplot(data = filter(melted_cormat, Var1 %in% c("First", "Second")), aes(x=Var1, y=Var2, fill=value)) + 
  labs(x ="Dose", y = "Date features") +
  geom_tile() +
   scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  geom_hline(yintercept = "Month", color = "blue", linetype = "dotted") +
  geom_hline(yintercept = "DayOfYear", color = "blue", linetype = "dotted") +
  geom_hline(yintercept = "Quarter", color = "blue", linetype = "dotted")
```

In the table below, we can see the numeric values.
```{r, echo=FALSE, fig.align = "center"}
kable(filter(melted_cormat, Var1 %in% c("First", "Second"), Var2 %in% c("Month", "DayOfYear", "Quarter")), col.names=NULL)
```

As we can see, the column "First" has a strong relationship with 

- "Quarter", 

- "DayOfYear",

- "Month". 


```{r, echo=FALSE}
ggplot(data = py$dataset, 
        aes(x=First, 
        y=Quarter)
) +
  geom_point() +
geom_smooth(method="lm", formula='y~x')
```

```{r, echo=FALSE}
ggplot(data = py$dataset, 
        aes(x=First, 
        y=DayOfYear)
) +
  geom_point() +
geom_smooth(method="lm", formula='y~x')
```

```{r, echo=FALSE}
ggplot(data = py$dataset, 
        aes(x=First, 
        y=Month)
) +
  geom_point() +
geom_smooth(method="lm", formula='y~x')
```

At the same time, the column "Second" doesn't have strong relationships; but we can use the same columns.

\newpage
#### Weekdays
As you remember, I have a question.
```{r ref0, ref.label='q2', render=pander::pander, echo=FALSE}
```
It may be helpful to choose the right features.

Let's answer.
```{r, echo=FALSE, fig.align = "center"}
dataset_weekdays <- 
  py$dataset %>%
  group_by(Weekday)%>%
  summarise(across(c("First", "Second", "Third"), ~ mean(.x, na.rm = TRUE)))

dataset_weekdays$Weekday <- factor(dataset_weekdays$Weekday, labels = c('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'))

dataset_weekdays_long <-
  melt(dataset_weekdays, id.vars = c("Weekday")
                       , measure.vars = c("First", "Second", "Third")
                       , variable.name = "Dose"
                       , value.name = "Count"
                    )

ggplot(data = dataset_weekdays_long, 
       aes(x=Weekday, 
           y=Count,
           fill=Count
           )
         ) +
  geom_col(position = "dodge") +
  geom_vline(xintercept = "Saturday", color = "red", linetype = "dotted") +
  facet_grid(Dose ~ .)
```

So, most of South West’s people prefer to get a jab on Saturdays. 
That is not illogical because, for example, for me, the side effects go away during the weekend.

\newpage
#### Missing values

As we already saw in the previous chapter \@ref(region), the column "Third" has missing values, but we can replace them with zeroes. Do we have the dates when nobody got the jab?

Calculate a count of dates in the dataset.
```{python, echo=FALSE}
len(dataset.index)
```
Calculate a count of dates between maximum and minimum dates.
```{python, echo=FALSE}
dates = pd.date_range(dataset.index.min(),dataset.index.max(),freq='d')
len(dates)
```
There are no missing dates.

So, we have finished the dataset exploring. The next steps are about the models.

\newpage
### Split sets, train a Machine Learning Model and Evaluate performance
```{r python_functions, include=FALSE}
# function for preparing sets
source_python('prepare_sets.py')
# function for training models
source_python('train_model.py')
```

**Define necessary variables**

First of all, I will use all columns that I have.
```{python all_columns, include=FALSE}
feature_columns = ["Year", "Month", "Day", "DayOfYear", "Weekday", "Quarter", "IsMonthStart", "IsMonthEnd"]
```

```{r, echo=FALSE}
my_kable(py$feature_columns)
```

**Prepare sets and train models using parameters.**
```{python first, include=FALSE}
y_column = "First"
```

The first column that I will predict is "First".

```{python train_all, echo=FALSE}
train_X, val_X, train_y, val_y = prepare_sets(dataset, feature_columns, y_column)

# DecisionTree
mae_dt, predictions_dt, model_dt = train_model(train_X, val_X, train_y, val_y, "DecisionTree", n_estimators=None)
# RandomForest
mae_rf, predictions_rf, model_rf = train_model(train_X, val_X, train_y, val_y, "RandomForest", n_estimators=500)

print("DecisionTree: ", 1 - mae_dt/dataset[y_column].mean())
print("RandomForest: ", 1 - mae_rf/dataset[y_column].mean())
```

Look at the tree

```{python tree_dt, echo=FALSE, fig.align = "center"}
plt.figure(figsize=(60, 40))
# feature_columns is defined above
tree.plot_tree(model_dt, max_depth=2, feature_names=feature_columns)
```

```{python tree_rf, echo=FALSE, fig.align = "center"}
plt.figure(figsize=(60,40))
# feature_columns is defined above
tree.plot_tree(model_rf.estimators_[0], max_depth=2, feature_names=feature_columns)
```

What can we see?



Finally, look at the result.

```{python plot_all, echo=FALSE, fig.align = "center"}
plt.figure(figsize=(25,15))
sns.set_style("darkgrid")
plt.title('{} ({})'.format("Comparing", y_column))
# plot DecisionTree
sns.lineplot(data=predictions_dt, label='{} MAE:{}'.format("DecisionTree", round(mae_dt,0)))
# plot RandomForest
sns.lineplot(data=predictions_rf, label='{} MAE:{}'.format("RandomForest", round(mae_rf,0)))
# plot validation set
val_y.index=range(0,len(val_y))
sns.lineplot(data=val_y, label="Validation")
# add legend
plt.legend()
```

In my opinion, the result is good.

- The waves were recognized.

- The extreme values are bigger than in real data.

Let’s work with the columns that I chose during the dataset exploring.

- "Weekday" that we discussed in this chapter influences the wave during the week.

- "Year" is the logical key because of the vaccination steps.

- DayOfYear was chosen because of the dependency on dates.


```{python my_columns, include=FALSE}
feature_columns = ["Weekday", "Year", "DayOfYear"]
```

```{python, echo=FALSE}
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

The result is better a little, but extreme values are disappointed.

Also, I suggest checking the model with columns that we discussed during the correlations search.

```{python correlations_columns, include=FALSE}
feature_columns = ["Month", "DayOfYear", "Quarter"]
```

```{python, echo=FALSE}
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

Not so good.

**Repeat for the Second**
```{python second_train, include=FALSE}
y_column = "Second"
```
```{python, echo=FALSE}
<<all_columns>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

\newpage
```{python, echo=FALSE}
<<my_columns>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

\newpage
```{python, echo=FALSE}
<<correlations_columns>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

**Repeat for Third**
```{python third_train, include=FALSE}
y_column = "Third"
```
```{python, echo=FALSE}
<<all_columns>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

\newpage
```{python, echo=FALSE}
<<my_columns>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

\newpage
```{python, echo=FALSE}
<<correlations_columns>>
<<train_all>>
#<<tree_dt>>
#<<tree_rf>>
<<plot_all>>
```

A combination of the following features give us the best result: Weekday, Year, DayOfYear.






